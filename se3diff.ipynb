{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.axes import Axes\n",
    "from torch._prims_common import DeviceLikeType\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from bioemu.so3_sde import angle_from_rotmat, rotmat_to_rotvec, rotvec_to_rotmat\n",
    "from bioemu.utils import clean_gpu_cache\n",
    "from se3diff.finetune import compute_finetune_loss, reverse_finetune_diffusion\n",
    "from se3diff.models import DiGMixSO3SDE, ScoreNet\n",
    "from se3diff.train import (\n",
    "    compute_train_loss,\n",
    "    igso3_mixture_marginal_pdf,\n",
    "    reverse_diffusion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sde = DiGMixSO3SDE()\n",
    "score_model = ScoreNet(rot_embed_dim=32, time_embed_dim=32, hidden_dim=128)\n",
    "# finetune_model = ScoreNet(rot_embed_dim=8, time_embed_dim=8, hidden_dim=32)\n",
    "finetune_model = ScoreNet(rot_embed_dim=16, time_embed_dim=16, hidden_dim=64)\n",
    "# finetune_model = ScoreNet(rot_embed_dim=32, time_embed_dim=32, hidden_dim=128)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model\n",
    "mus = rotvec_to_rotmat(\n",
    "    torch.tensor([[0.0, 0.0, 0.0], [0.0, math.pi / 2, 0.0], [0.0, 0.0, math.pi]])\n",
    ")\n",
    "sigmas = torch.tensor([0.2, 0.1, 0.3])\n",
    "# weights = torch.tensor([0.3, 0.1, 0.6])\n",
    "weights = torch.tensor([0.3, 0.4, 0.3])\n",
    "print(f\"Original model: \\n{mus}\")\n",
    "\n",
    "# Fine-tuned model\n",
    "# mus_finetune = rotvec_to_rotmat(\n",
    "#     torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, math.pi]]) + torch.randn(2, 3) * 0.05\n",
    "# )\n",
    "# sigmas_finetune = torch.tensor([0.2, 0.1])\n",
    "# weights_finetune = torch.tensor([0.6, 0.4])\n",
    "mus_finetune = mus.clone()\n",
    "sigmas_finetune = sigmas.clone()\n",
    "weights_finetune = torch.tensor([0.4, 0.2, 0.4])\n",
    "print(f\"Fine-tuned model: \\n{mus_finetune}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original model\n",
    "# mus = rotvec_to_rotmat(torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, math.pi]]))\n",
    "# sigmas = torch.tensor([0.1, 0.2])\n",
    "# weights = torch.tensor([0.1, 0.9])\n",
    "# print(f\"Original model: \\n{mus}\")\n",
    "\n",
    "# # Fine-tuned model\n",
    "# mus_finetune = mus.clone()\n",
    "# sigmas_finetune = sigmas.clone()\n",
    "# weights_finetune = torch.tensor([0.3, 0.7])\n",
    "# print(f\"Fine-tuned model: \\n{mus_finetune}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_initial_settings(\n",
    "    mus: torch.Tensor,\n",
    "    sigmas: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    mus_finetune: torch.Tensor,\n",
    "    sigmas_finetune: torch.Tensor,\n",
    "    weights_finetune: torch.Tensor,\n",
    ") -> None:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.set_title(\"Initial Settings\")\n",
    "    omega, pdf = igso3_mixture_marginal_pdf(mus, sigmas, weights)\n",
    "    omega_finetune, pdf_finetune = igso3_mixture_marginal_pdf(\n",
    "        mus_finetune, sigmas_finetune, weights_finetune\n",
    "    )\n",
    "    ax.plot(\n",
    "        omega.cpu().numpy(),\n",
    "        pdf.cpu().numpy(),\n",
    "        color=\"palegreen\",\n",
    "        lw=2,\n",
    "        label=\"Original Mixture\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        omega_finetune.cpu().numpy(),\n",
    "        pdf_finetune.cpu().numpy(),\n",
    "        color=\"lightblue\",\n",
    "        lw=2,\n",
    "        label=\"Fine-tuned Mixture\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Angle (rad)\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_initial_settings(\n",
    "    mus,\n",
    "    sigmas,\n",
    "    weights,\n",
    "    mus_finetune,\n",
    "    sigmas_finetune,\n",
    "    weights_finetune,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    sde: DiGMixSO3SDE,\n",
    "    score_model: nn.Module,\n",
    "    mus: torch.Tensor,\n",
    "    sigmas: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    *,\n",
    "    device: DeviceLikeType | None = None,\n",
    "    output_dir: str = \".\",\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 4096,\n",
    "    num_batches: int = 200,\n",
    "    lr: float = 5e-3,\n",
    "    weight_decay: float = 1e-4,\n",
    "    eta_min: float = 1e-5,\n",
    "    tol: float = 1e-7,\n",
    "):\n",
    "    # instantiate SDE and score model\n",
    "    sde.to(device)\n",
    "    score_model.to(device)\n",
    "\n",
    "    # instantiate the mixture parameters\n",
    "    mus = mus.to(device)\n",
    "    sigmas = sigmas.to(device)\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    optimizer = AdamW(score_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=eta_min)\n",
    "\n",
    "    best_model = {}\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        score_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch}\", leave=False)\n",
    "        for _ in pbar:\n",
    "            loss = compute_train_loss(\n",
    "                sde,\n",
    "                score_model,\n",
    "                mus,\n",
    "                sigmas,\n",
    "                weights,\n",
    "                device=device,\n",
    "                batch_size=batch_size,\n",
    "                tol=tol,\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            l = loss.detach().item()\n",
    "            epoch_loss += l\n",
    "            pbar.set_postfix(loss=f\"{l:.2f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_model = score_model.state_dict()\n",
    "            print(f\"Updated best model at epoch {epoch}\")\n",
    "        print(f\"Epoch {epoch}: Average training loss = {avg_loss:.4f}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            score_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = compute_train_loss(\n",
    "                    sde,\n",
    "                    score_model,\n",
    "                    mus,\n",
    "                    sigmas,\n",
    "                    weights,\n",
    "                    device=device,\n",
    "                    batch_size=batch_size,\n",
    "                    tol=tol,\n",
    "                )\n",
    "            print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "            torch.save(\n",
    "                score_model.state_dict(),\n",
    "                os.path.join(output_dir, f\"score_model_{epoch}.pt\"),\n",
    "            )\n",
    "            print(\n",
    "                f\"Model saved to {os.path.join(output_dir, f'score_model_{epoch}.pt')}\"\n",
    "            )\n",
    "\n",
    "    score_model.load_state_dict(best_model)\n",
    "    torch.save(score_model.state_dict(), os.path.join(output_dir, \"score_model.pt\"))\n",
    "\n",
    "\n",
    "train(\n",
    "    sde,\n",
    "    score_model,\n",
    "    mus,\n",
    "    sigmas,\n",
    "    weights,\n",
    "    device=device,\n",
    "    output_dir=\"score_models\",\n",
    ")\n",
    "# save the model\n",
    "torch.save(score_model.state_dict(), \"score_model.pt\")\n",
    "# load the model\n",
    "score_model.load_state_dict(torch.load(\"score_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_gpu_cache\n",
    "@torch.no_grad()\n",
    "def visualize_diffusion(\n",
    "    sde: DiGMixSO3SDE,\n",
    "    score_model: nn.Module,\n",
    "    mus: torch.Tensor,\n",
    "    sigmas: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    *,\n",
    "    device: DeviceLikeType | None = None,\n",
    "    batch_size: int = 65536,\n",
    "    num_steps: int = 2000,\n",
    "    t_num: int = 8,\n",
    "    l_max: int = 1000,\n",
    "    num_points: int = 1000,\n",
    "    tol: float = 1e-7,\n",
    "):\n",
    "    sde.to(device)\n",
    "    score_model.to(device).eval()\n",
    "\n",
    "    mus = mus.to(device)\n",
    "    sigmas = sigmas.to(device)\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    # reverse diffusion samples\n",
    "    xs, timesteps = reverse_diffusion(\n",
    "        sde, score_model, device=device, batch_size=batch_size, num_steps=num_steps\n",
    "    )\n",
    "\n",
    "    x_0 = sde.sample_multiple_igso3(\n",
    "        mus, sigmas, weights, batch_size, device=device\n",
    "    )  # (B,3,3)\n",
    "\n",
    "    # pick checkpoints\n",
    "    idxs = np.linspace(0, num_steps, t_num, dtype=int)\n",
    "    fig, axes = plt.subplots(\n",
    "        2, int(np.ceil(t_num / 2)), figsize=(15, 6), constrained_layout=True\n",
    "    )\n",
    "    axes: np.ndarray = axes.flatten()\n",
    "\n",
    "    for i in trange(t_num, desc=\"Plotting\", leave=False):\n",
    "        # plot forward and reverse marginals\n",
    "        ax: Axes = axes[i]\n",
    "        idx = idxs[i]\n",
    "        timestep = timesteps[idx].item()\n",
    "        t = torch.full((batch_size,), timestep, device=device)\n",
    "\n",
    "        # a) true forward histogram\n",
    "        x_t = sde.sample_marginal(x_0, t)\n",
    "        true_angles = angle_from_rotmat(x_t)[0]\n",
    "        ax.hist(\n",
    "            true_angles.cpu().numpy(),\n",
    "            bins=100,\n",
    "            density=True,\n",
    "            alpha=0.8,\n",
    "            label=\"true forward\",\n",
    "            color=\"lightblue\",\n",
    "        )\n",
    "\n",
    "        # b) predicted reverse histogram\n",
    "        x_rev = xs[idx]\n",
    "        rev_angles = angle_from_rotmat(x_rev)[0]\n",
    "        ax.hist(\n",
    "            rev_angles.cpu().numpy(),\n",
    "            bins=100,\n",
    "            density=True,\n",
    "            histtype=\"step\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1,\n",
    "            label=\"predicted reverse\",\n",
    "            color=\"orange\",\n",
    "        )\n",
    "\n",
    "        # c) theoretical forward marginal (vectorized)\n",
    "        sigma_t = torch.sqrt(sde._marginal_std(timesteps[idx]) ** 2 + sigmas**2)  # (K,)\n",
    "        omega, pdf = igso3_mixture_marginal_pdf(\n",
    "            mus,\n",
    "            sigma_t,\n",
    "            weights,\n",
    "            l_max=l_max,\n",
    "            num_points=num_points,\n",
    "            tol=tol,\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            omega.cpu().numpy(),\n",
    "            pdf.cpu().numpy(),\n",
    "            color=\"pink\",\n",
    "            lw=2,\n",
    "            label=\"theoretical PDF\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"t = {timestep:.2f}\")\n",
    "        ax.set_xlabel(\"rotation-angle (rad)\")\n",
    "        ax.set_ylabel(\"density\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "score_model.load_state_dict(torch.load(\"score_model.pt\", map_location=device))\n",
    "visualize_diffusion(sde, score_model, mus, sigmas, weights, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    sde: DiGMixSO3SDE,\n",
    "    score_model: nn.Module,\n",
    "    finetune_model: nn.Module,\n",
    "    mus: torch.Tensor,\n",
    "    sigmas: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    *,\n",
    "    device: DeviceLikeType | None = None,\n",
    "    output_dir: str = \".\",\n",
    "    lambda_: float = 0.1,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 4096,\n",
    "    num_batches: int = 4,\n",
    "    num_steps: int = 200,\n",
    "    l_max: int = 1000,\n",
    "    lr: float = 5e-3,\n",
    "    weight_decay: float = 1e-4,\n",
    "    eta_min: float = 1e-5,\n",
    "    tol: float = 1e-7,\n",
    "):\n",
    "    # instantiate SDE and score model\n",
    "    sde.to(device)\n",
    "    score_model.to(device).eval()\n",
    "    finetune_model.to(device)\n",
    "\n",
    "    # instantiate the mixture parameters\n",
    "    mus = mus.to(device)\n",
    "    sigmas = sigmas.to(device)\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    optimizer = AdamW(finetune_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=eta_min)\n",
    "\n",
    "    best_model = {}\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        finetune_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch}\", leave=False)\n",
    "        for _ in pbar:\n",
    "            loss = compute_finetune_loss(\n",
    "                sde,\n",
    "                score_model,\n",
    "                finetune_model,\n",
    "                mus,\n",
    "                sigmas,\n",
    "                weights,\n",
    "                lambda_=lambda_,\n",
    "                device=device,\n",
    "                batch_size=batch_size,\n",
    "                num_steps=num_steps,\n",
    "                l_max=l_max,\n",
    "                tol=tol,\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            l = loss.detach().item()\n",
    "            epoch_loss += l\n",
    "            pbar.set_postfix(loss=f\"{l:.2f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_model = finetune_model.state_dict()\n",
    "            print(f\"Updated best model at epoch {epoch}\")\n",
    "        print(f\"Epoch {epoch}: Average training loss = {avg_loss:.4f}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            finetune_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = compute_finetune_loss(\n",
    "                    sde,\n",
    "                    score_model,\n",
    "                    finetune_model,\n",
    "                    mus,\n",
    "                    sigmas,\n",
    "                    weights,\n",
    "                    lambda_=lambda_,\n",
    "                    device=device,\n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    l_max=l_max,\n",
    "                    tol=tol,\n",
    "                )\n",
    "            print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "            torch.save(\n",
    "                finetune_model.state_dict(),\n",
    "                os.path.join(output_dir, f\"finetune_model_{epoch}.pt\"),\n",
    "            )\n",
    "            print(\n",
    "                f\"Model saved to {os.path.join(output_dir, f'finetune_model_{epoch}.pt')}\"\n",
    "            )\n",
    "\n",
    "    finetune_model.load_state_dict(best_model)\n",
    "    torch.save(\n",
    "        finetune_model.state_dict(), os.path.join(output_dir, \"finetune_model.pt\")\n",
    "    )\n",
    "\n",
    "\n",
    "score_model.load_state_dict(torch.load(\"score_model.pt\", map_location=device))\n",
    "finetune(\n",
    "    sde,\n",
    "    score_model,\n",
    "    finetune_model,\n",
    "    mus_finetune,\n",
    "    sigmas_finetune,\n",
    "    weights_finetune,\n",
    "    device=device,\n",
    "    output_dir=\"finetune_models\",\n",
    "    lambda_=0.1,\n",
    ")\n",
    "# save the model\n",
    "torch.save(finetune_model.state_dict(), \"finetune_model.pt\")\n",
    "# load the model\n",
    "finetune_model.load_state_dict(torch.load(\"finetune_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_gpu_cache\n",
    "@torch.no_grad()\n",
    "def visualize_finetune_diffusion(\n",
    "    sde: DiGMixSO3SDE,\n",
    "    score_model: nn.Module,\n",
    "    finetune_model: nn.Module,\n",
    "    mus: torch.Tensor,\n",
    "    sigmas: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    *,\n",
    "    device: DeviceLikeType | None = None,\n",
    "    batch_size: int = 65536,\n",
    "    num_steps: int = 2000,\n",
    "    t_num: int = 8,\n",
    "    l_max: int = 1000,\n",
    "    num_points: int = 1000,\n",
    "    tol: float = 1e-7,\n",
    "):\n",
    "    sde.to(device)\n",
    "    score_model.to(device).eval()\n",
    "    finetune_model.to(device).eval()\n",
    "\n",
    "    mus = mus.to(device)\n",
    "    sigmas = sigmas.to(device)\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    # reverse diffusion samples\n",
    "    xs, timesteps, _ = reverse_finetune_diffusion(\n",
    "        sde,\n",
    "        score_model,\n",
    "        finetune_model,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "\n",
    "    x_0 = sde.sample_multiple_igso3(\n",
    "        mus, sigmas, weights, batch_size, device=device\n",
    "    )  # (B,3,3)\n",
    "\n",
    "    # pick checkpoints\n",
    "    idxs = np.linspace(0, num_steps, t_num, dtype=int)\n",
    "    fig, axes = plt.subplots(\n",
    "        2, int(np.ceil(t_num / 2)), figsize=(15, 6), constrained_layout=True\n",
    "    )\n",
    "    axes: np.ndarray = axes.flatten()\n",
    "\n",
    "    for i in trange(t_num, desc=\"Plotting\", leave=False):\n",
    "        # plot forward and reverse marginals\n",
    "        ax: Axes = axes[i]\n",
    "        idx = idxs[i]\n",
    "        timestep = timesteps[idx].item()\n",
    "        t = torch.full((batch_size,), timestep, device=device)\n",
    "\n",
    "        # a) true forward histogram\n",
    "        x_t = sde.sample_marginal(x_0, t)\n",
    "        true_angles = angle_from_rotmat(x_t)[0]\n",
    "        ax.hist(\n",
    "            true_angles.cpu().numpy(),\n",
    "            bins=100,\n",
    "            density=True,\n",
    "            alpha=0.8,\n",
    "            label=\"true forward\",\n",
    "            color=\"lightblue\",\n",
    "        )\n",
    "\n",
    "        # b) predicted reverse histogram\n",
    "        x_rev = xs[idx]\n",
    "        rev_angles = angle_from_rotmat(x_rev)[0]\n",
    "        ax.hist(\n",
    "            rev_angles.cpu().numpy(),\n",
    "            bins=100,\n",
    "            density=True,\n",
    "            histtype=\"step\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1,\n",
    "            label=\"predicted reverse\",\n",
    "            color=\"orange\",\n",
    "        )\n",
    "\n",
    "        # c) theoretical forward marginal (vectorized)\n",
    "        sigma_t = torch.sqrt(sde._marginal_std(timesteps[idx]) ** 2 + sigmas**2)  # (K,)\n",
    "        omega, pdf = igso3_mixture_marginal_pdf(\n",
    "            mus,\n",
    "            sigma_t,\n",
    "            weights,\n",
    "            l_max=l_max,\n",
    "            num_points=num_points,\n",
    "            tol=tol,\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            omega.cpu().numpy(),\n",
    "            pdf.cpu().numpy(),\n",
    "            color=\"pink\",\n",
    "            lw=2,\n",
    "            label=\"theoretical PDF\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"t = {timestep:.2f}\")\n",
    "        ax.set_xlabel(\"rotation-angle (rad)\")\n",
    "        ax.set_ylabel(\"density\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "score_model.load_state_dict(torch.load(\"score_model.pt\", map_location=device))\n",
    "finetune_model.load_state_dict(torch.load(\"finetune_model.pt\", map_location=device))\n",
    "visualize_finetune_diffusion(\n",
    "    sde,\n",
    "    score_model,\n",
    "    finetune_model,\n",
    "    mus_finetune,\n",
    "    sigmas_finetune,\n",
    "    weights_finetune,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RSVrXQpEYPYz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "se3diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
